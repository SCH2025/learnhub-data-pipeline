#!/usr/bin/env python3
"""
ETL Pipeline: PostgreSQL â†’ GCS
å¾ PostgreSQL æŠ½å–æ•¸æ“šä¸¦ä¸Šå‚³åˆ° Google Cloud Storage (Parquet æ ¼å¼)
"""

import os
import psycopg2
import pandas as pd
from google.cloud import storage
from datetime import datetime
import logging
from pathlib import Path

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================
# é…ç½®
# ============================================
PG_CONFIG = {
    'host': 'localhost',
    'port': 5433,
    'database': 'learnhub_prod',
    'user': 'admin',
    'password': 'admin123'
}

GCS_BUCKET = 'learnhub-raw-data-2025-0112'  
GCS_PREFIX = 'raw/'

# è¨­å®š Service Account é‡‘é‘°è·¯å¾‘
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = './config/gcp/service-account-key.json'

# ============================================
# è¦æŠ½å–çš„è³‡æ–™è¡¨
# ============================================
TABLES = [
    'users',
    'subscriptions',
    'courses',
    'instructors',
    'course_categories',
    'subscription_plans',
    'payments',
    'course_enrollments'
]

# ============================================
# æŠ½å–æ•¸æ“š
# ============================================
def extract_table(conn, table_name):
    """å¾ PostgreSQL æŠ½å–å–®ä¸€è³‡æ–™è¡¨"""
    logger.info(f"ğŸ“¥ æŠ½å–è³‡æ–™è¡¨ï¼š{table_name}")
    
    query = f"SELECT * FROM {table_name};"
    df = pd.read_sql(query, conn)
    
    logger.info(f"  âœ… æŠ½å–å®Œæˆï¼š{len(df):,} ç­†è¨˜éŒ„")
    return df

# ============================================
# ä¸Šå‚³åˆ° GCS
# ============================================
def upload_to_gcs(df, table_name, bucket_name, prefix):
    """ä¸Šå‚³ DataFrame åˆ° GCSï¼ˆParquet æ ¼å¼ï¼‰"""
    logger.info(f"â˜ï¸  ä¸Šå‚³åˆ° GCSï¼š{table_name}")
    
    # ç”Ÿæˆæª”æ¡ˆåç¨±ï¼ˆå«æ—¥æœŸï¼‰
    date_str = datetime.now().strftime('%Y%m%d')
    filename = f"{table_name}_{date_str}.parquet"
    
    # æš«å­˜åˆ°æœ¬åœ°
    local_path = f"/tmp/{filename}"
    df.to_parquet(local_path, index=False, compression='snappy', engine='pyarrow', coerce_timestamps='us', allow_truncated_timestamps=True)
    
    # ä¸Šå‚³åˆ° GCS
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob_path = f"{prefix}{table_name}/{filename}"
    blob = bucket.blob(blob_path)
    
    blob.upload_from_filename(local_path)
    
    # æ¸…ç†æš«å­˜æª”æ¡ˆ
    os.remove(local_path)
    
    logger.info(f"  âœ… ä¸Šå‚³å®Œæˆï¼šgs://{bucket_name}/{blob_path}")
    logger.info(f"  ğŸ“Š æª”æ¡ˆå¤§å°ï¼š{blob.size / 1024 / 1024:.2f} MB")
    
    return blob_path

# ============================================
# ä¸»ç¨‹å¼
# ============================================
def main():
    logger.info("=" * 60)
    logger.info("ETL Pipeline: PostgreSQL â†’ GCS")
    logger.info("=" * 60)
    
    try:
        # é€£æ¥ PostgreSQL
        logger.info("\nğŸ”Œ é€£æ¥ PostgreSQL...")
        conn = psycopg2.connect(**PG_CONFIG)
        logger.info("âœ… PostgreSQL é€£ç·šæˆåŠŸ")
        
        # æ¸¬è©¦ GCS é€£ç·š
        logger.info("\nğŸ”Œ æ¸¬è©¦ GCS é€£ç·š...")
        client = storage.Client()
        bucket = client.bucket(GCS_BUCKET)
        if bucket.exists():
            logger.info(f"âœ… GCS Bucket å­˜åœ¨ï¼š{GCS_BUCKET}")
        else:
            raise Exception(f"âŒ GCS Bucket ä¸å­˜åœ¨ï¼š{GCS_BUCKET}")
        
        # é–‹å§‹ ETL
        logger.info("\nğŸš€ é–‹å§‹ ETL æµç¨‹...")
        logger.info(f"å°‡æŠ½å– {len(TABLES)} å€‹è³‡æ–™è¡¨\n")
        
        results = []
        
        for table in TABLES:
            try:
                # æŠ½å–
                df = extract_table(conn, table)
                
                # ä¸Šå‚³
                blob_path = upload_to_gcs(df, table, GCS_BUCKET, GCS_PREFIX)
                
                results.append({
                    'table': table,
                    'rows': len(df),
                    'status': 'success',
                    'path': blob_path
                })
                
                logger.info("")  # ç©ºè¡Œåˆ†éš”
                
            except Exception as e:
                logger.error(f"âŒ è™•ç† {table} æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                results.append({
                    'table': table,
                    'rows': 0,
                    'status': 'failed',
                    'error': str(e)
                })
        
        # é—œé–‰é€£ç·š
        conn.close()
        
        # ç¸½çµ
        logger.info("=" * 60)
        logger.info("ETL å®Œæˆç¸½çµ")
        logger.info("=" * 60)
        
        success_count = sum(1 for r in results if r['status'] == 'success')
        total_rows = sum(r['rows'] for r in results if r['status'] == 'success')
        
        logger.info(f"âœ… æˆåŠŸï¼š{success_count}/{len(TABLES)} å€‹è³‡æ–™è¡¨")
        logger.info(f"ğŸ“Š ç¸½è¨ˆï¼š{total_rows:,} ç­†è¨˜éŒ„")
        
        if success_count < len(TABLES):
            logger.warning(f"âš ï¸  å¤±æ•—ï¼š{len(TABLES) - success_count} å€‹è³‡æ–™è¡¨")
        
        logger.info("\nè©³ç´°çµæœï¼š")
        for r in results:
            status_icon = "âœ…" if r['status'] == 'success' else "âŒ"
            logger.info(f"  {status_icon} {r['table']}: {r['rows']:,} ç­†")
        
    except Exception as e:
        logger.error(f"âŒ ETL å¤±æ•—ï¼š{e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == '__main__':
    exit(main())